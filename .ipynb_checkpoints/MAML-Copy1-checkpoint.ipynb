{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dc7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d64468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 362, 6])\n"
     ]
    }
   ],
   "source": [
    "#  读入传感器数据\n",
    "df_train = pd.read_csv('train_data.csv')\n",
    "num_train = df_train['Unit'].max()\n",
    "\n",
    "targets = []\n",
    "\n",
    "for i in range(num_train):\n",
    "    sensor_data = torch.tensor(df_train[df_train['Unit'] == i+1].iloc[:,2:].values, dtype = torch.float)  \n",
    "    targets.append(sensor_data)\n",
    "\n",
    "# 记录传感器数据的真实长度\n",
    "lengths = []\n",
    "for i in range(num_train):\n",
    "    lengths.append(targets[i].shape[0])\n",
    "    \n",
    "# 对不等长的传感器数据进行填充\n",
    "pad_targets = pad_sequence(targets, batch_first=True, padding_value=0.0)\n",
    "print(pad_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0510c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_task = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffad8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_x_batch = []\n",
    "query_x_batch = []\n",
    "selected_units = np.random.choice(num_train, num_task, False)\n",
    "for i in range(num_task):\n",
    "    index = selected_units[i]\n",
    "    valid_len = lengths[index]\n",
    "    query_x_batch.append(targets[index])\n",
    "    \n",
    "    a = 500\n",
    "    while a > valid_len:\n",
    "        a = int(np.random.uniform(31,303))\n",
    "        \n",
    "    support_x_batch.append(targets[index][:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7a2b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "184\n",
      "113\n",
      "33\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "for support_x in support_x_batch:\n",
    "    print(len(support_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9663869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "234\n",
      "188\n",
      "208\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "for query_x in query_x_batch:\n",
    "    print(len(query_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43452a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 1000\n",
    "num_units, k_shot = num_train, 2\n",
    "\n",
    "support_x_batch = []\n",
    "query_x_batch = []\n",
    "\n",
    "for b in range(batchsz):   # 采样batchsz个task\n",
    "    selected_unit = np.random.choice(num_units, 1, False)[0]\n",
    "\n",
    "    valid_len = lengths[selected_unit]\n",
    "    query_x = targets[selected_unit]\n",
    "    support_x = []\n",
    "\n",
    "    for i in range(k_shot):\n",
    "        a = 500\n",
    "        while a > valid_len:\n",
    "            a = int(np.random.uniform(31,303))\n",
    "        support_x.append(targets[selected_unit][:a])\n",
    "\n",
    "    support_x_batch.append(support_x)\n",
    "    query_x_batch.append(query_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f7329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(support_x_batch), len(query_x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba0f1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 95, 163)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(support_x_batch[0][0]), len(support_x_batch[0][1]), len(query_x_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be97f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit(Dataset):\n",
    "    def __init__(self, batchsz, k_shot, data, lengths):\n",
    "        self.batchsz = batchsz\n",
    "        self.data = data\n",
    "        self.lengths = lengths\n",
    "        \n",
    "        self.num_units = len(data)\n",
    "        self.creat_batch(self.batchsz)\n",
    "    \n",
    "    def creat_batch(self, batchsz):\n",
    "        self.support_x_batch = []\n",
    "        self.support_len_batch = []\n",
    "        \n",
    "        self.query_x_batch = []\n",
    "        self.query_len_batch = []\n",
    "        \n",
    "        for b in range(batchsz):   # 采样batchsz个task\n",
    "            selected_unit = np.random.choice(self.num_units, 1, False)[0]\n",
    "            \n",
    "            valid_len = self.lengths[selected_unit]\n",
    "            \n",
    "            query_x = self.data[selected_unit]\n",
    "            support_x = self.data[selected_unit]\n",
    "            support_len = valid_len            \n",
    "            \n",
    "            a = 500\n",
    "            while a > valid_len:\n",
    "                a = int(np.random.uniform(31,303))\n",
    "            query_len = a\n",
    "            \n",
    "            self.support_x_batch.append(support_x)\n",
    "            self.query_x_batch.append(query_x)\n",
    "            \n",
    "            self.support_len_batch.append(support_len)\n",
    "            self.query_len_batch.append(query_len)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.support_x_batch[index], self.query_x_batch[index], self.support_len_batch[index], self.query_len_batch[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batchsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "914633b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz, k_shot = 100, 2\n",
    "dataset = Unit(batchsz, k_shot, pad_targets, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d0db44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_task = 5\n",
    "train_loader = DataLoader(dataset, num_task, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d76a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([217, 313, 137, 163, 185])\n",
      "tensor([207, 207, 153, 217, 217])\n",
      "tensor([154, 207, 199, 154, 207])\n",
      "tensor([258, 189, 216, 231, 258])\n",
      "tensor([202, 207, 137, 202, 240])\n",
      "tensor([202, 230, 188, 194, 163])\n",
      "tensor([213, 362, 195, 283, 185])\n",
      "tensor([178, 215, 137, 240, 199])\n",
      "tensor([158, 192, 283, 214, 185])\n",
      "tensor([213, 195, 313, 180, 188])\n",
      "tensor([194, 128, 202, 234, 137])\n",
      "tensor([188, 240, 213, 180, 213])\n",
      "tensor([283, 174, 170, 213, 170])\n",
      "tensor([276, 275, 336, 216, 275])\n",
      "tensor([213, 213, 287, 199, 234])\n",
      "tensor([287, 199, 217, 313, 199])\n",
      "tensor([195, 278, 154, 153, 229])\n",
      "tensor([202, 185, 188, 185, 174])\n",
      "tensor([259, 172, 231, 198, 147])\n",
      "tensor([180, 158, 213, 213, 180])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch_units in enumerate(train_loader):\n",
    "    support_x_batch, query_x_batch, support_len_batch, query_len_batch = batch_units\n",
    "    print(support_len_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6f7e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 180, 188, 213, 163])\n",
      "tensor([195, 202, 259, 217, 194])\n",
      "tensor([198, 336, 287, 188, 313])\n",
      "tensor([313, 199, 128, 163, 195])\n",
      "tensor([275, 178, 195, 258, 189])\n",
      "tensor([213, 192, 216, 170, 154])\n",
      "tensor([229, 283, 199, 207, 240])\n",
      "tensor([213, 199, 207, 202, 153])\n",
      "tensor([213, 202, 287, 158, 217])\n",
      "tensor([207, 216, 188, 185, 137])\n",
      "tensor([202, 283, 199, 170, 188])\n",
      "tensor([275, 207, 180, 174, 202])\n",
      "tensor([231, 154, 180, 147, 207])\n",
      "tensor([258, 174, 213, 185, 240])\n",
      "tensor([185, 213, 154, 215, 137])\n",
      "tensor([137, 240, 153, 194, 234])\n",
      "tensor([231, 158, 199, 217, 217])\n",
      "tensor([230, 185, 172, 213, 137])\n",
      "tensor([276, 234, 213, 362, 180])\n",
      "tensor([283, 214, 213, 313, 278])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, num_task, shuffle=True)\n",
    "for batch_idx, batch_units in enumerate(train_loader):\n",
    "    support_x_batch, query_x_batch, support_len_batch, query_len_batch = batch_units\n",
    "    print(support_len_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f4e747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(support_x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2345a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_x_batch.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71ff57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, num_hiddens, num_outputs):\n",
    "        self.Psi = torch.load('Psi.params')\n",
    "        self.gamma = Variable(nn.Parameter(torch.randn(num_hiddens, num_outputs))) * 0.01\n",
    "        \n",
    "    def __call__(self, gamma=None):\n",
    "        if gamma is None:\n",
    "            gamma = self.gamma\n",
    "        return Psi @ gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65d27395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    def __init__(self, update_lr, meta_lr, k_spt, num_task, update_step, update_step_test):\n",
    "        super(Meta, self).__init__()\n",
    "        \n",
    "        self.update_lr = update_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.k_spt = k_spt\n",
    "        self.num_task = num_task\n",
    "        self.update_step = update_step\n",
    "        self.update_test_step = update_test_step\n",
    "        \n",
    "        self.net = LSTM()\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "        \n",
    "    def forward(self, x_spt, len_spt, x_qry, len_qry):\n",
    "        num_task = x_spt.size()[0]\n",
    "        \n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]\n",
    "        \n",
    "        for i in range(num_task):\n",
    "            pred_y = self.net(self.net.gamma)[:len_spt[i]]\n",
    "            true_y = x_spt[:len_spt[i]]\n",
    "            \n",
    "            loss_q = mse_loss(pred_y, true_y)\n",
    "            grad = torch.autograd.grad(loss, self.net.gamma)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.gamma)))\n",
    "            \n",
    "            # this is the loss before first update\n",
    "            with torch.no_grad():\n",
    "                pred_y = self.net(self.net.gamma)[:len_qry[i]]\n",
    "                true_y = x_qry[:len_qry[i]]\n",
    "                loss_q = mse_loss(pred_y, true_y)\n",
    "                losses_q[0] += loss_q\n",
    "             \n",
    "            # this is the loss after the first update\n",
    "            with torch.no_grad():\n",
    "                pred_y = self.net(fast_weights)[:len_qry[i]]\n",
    "                true_y = x_qry[:len_qry[i]]\n",
    "                loss_q = mse_loss(pred_y, true_y)\n",
    "                losses_q[1] += loss_q\n",
    "                \n",
    "            for k in range(1, self.update_step):\n",
    "                pred_y = self.net(fast_weights)[:len_spt[i]]\n",
    "                true_y = x_spt[:len_spt[i]]\n",
    "                loss = mse_loss(pred_y, true_y)\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.gamma)))\n",
    "                \n",
    "                pred_y = self.net(fast.weights)[:len_qry[i]]\n",
    "                true_y = x_qry[:len_qry[i]]\n",
    "                loss_q = mse_loss(pred_y, true_y)\n",
    "                losses_q[k+1] += loss_q\n",
    "                \n",
    "        loss_q = losses_q[-1] / task_num\n",
    "          \n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "        self.meta_optim.step() \n",
    "        \n",
    "    def finetunning(self, x_spt, len_spt):\n",
    "        net = deepcopy(self.net)\n",
    "        \n",
    "        pred_y = net()[:len_spt]\n",
    "        true_y = x_spt\n",
    "        loss = mse_loss(pred_y, true_y)\n",
    "        grad = torch.autograd.grad(loss, net.gamma)\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.gamma)))\n",
    "        \n",
    "        for k in range(1, self.update_step_test):\n",
    "            pred_y = net()[:len_spt]\n",
    "            true_y = x_spt\n",
    "            loss = mse_loss(pred_y, true_y)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "        \n",
    "        del net\n",
    "        return fast_weights           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_lr, meta_lr = \n",
    "num_task = \n",
    "updaye_step, update_step_test = \n",
    "maml = Meta(update_lr, meta_lr, num_task, update_step, update_step_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
