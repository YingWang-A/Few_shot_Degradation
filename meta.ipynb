{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d883f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "\n",
    "from    learner import Learner\n",
    "from    copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d120e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = args.update_lr        # 内循环的学习率\n",
    "        self.meta_lr = args.meta_lr            # 外循环的学习率\n",
    "        self.n_way = args.n_way\n",
    "        self.k_spt = args.k_spt\n",
    "        self.k_qry = args.k_qry\n",
    "        self.task_num = args.task_num\n",
    "        self.update_step = args.update_step    # 内循环的迭代次数\n",
    "        self.update_step_test = args.update_step_test    # 微调的迭代次数\n",
    "\n",
    "\n",
    "        self.net = Learner(config, args.imgc, args.imgsz)\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def clip_grad_by_norm_(self, grad, max_norm):\n",
    "        \"\"\"\n",
    "        in-place gradient clipping.\n",
    "        :param grad: list of gradients\n",
    "        :param max_norm: maximum norm allowable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        total_norm = 0\n",
    "        counter = 0\n",
    "        for g in grad:\n",
    "            param_norm = g.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            counter += 1\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for g in grad:\n",
    "                g.data.mul_(clip_coef)\n",
    "\n",
    "        return total_norm/counter\n",
    "\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(x_spt[i], vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt[i])\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # 在 with torch.no_grad()模块下 所有计算得出的tensor的requires_grad都自动设置为False\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[0] += loss_q\n",
    "\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[1] += loss_q\n",
    "                # [setsz]\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item() # 分类正确的数量\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(x_spt[i], fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, y_spt[i])\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_q, y_qry[i]).sum().item()  # convert to numpy\n",
    "                    corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "        # print('meta update')\n",
    "        # for p in self.net.parameters()[:5]:\n",
    "        # \tprint(torch.norm(p).item())\n",
    "        self.meta_optim.step()\n",
    "\n",
    "\n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "\n",
    "        return accs\n",
    "\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [setsz, c_, h, w]\n",
    "        :param y_spt:   [setsz]\n",
    "        :param x_qry:   [querysz, c_, h, w]\n",
    "        :param y_qry:   [querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        querysz = x_qry.size(0)\n",
    "\n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "\n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(x_spt)\n",
    "        loss = F.cross_entropy(logits, y_spt)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, net.parameters(), bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[0] = corrects[0] + correct\n",
    "\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[1] = corrects[1] + correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(x_spt, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, y_qry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry).sum().item()  # convert to numpy\n",
    "                corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "        del net\n",
    "\n",
    "        accs = np.array(corrects) / querysz\n",
    "\n",
    "        return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf60762",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = [0.1, 0.2, 0.3, 0.4]\n",
    "params = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee893a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_weights = list(map(lambda p: p[1] - p[0], zip(grad, params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5b105a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9, 1.8, 2.7, 3.6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30dc5966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.2, 0.3, 0.4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905089bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
